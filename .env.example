# LLM Configuration
LLM_PROVIDER=openai  # "openai", "gemini", or "openrouter"

# OpenAI
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1  # Use custom endpoint if needed
OPENAI_MODEL=gpt-4o-mini  # Recommended economical default

# Gemini
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-1.5-flash-latest

# OpenRouter
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=openai/gpt-4o-mini  # Recommended economical default

# Backup two-phase fallback (app/tools/backup_script.py)
# Separate OpenAI-compatible endpoint for backup workflow
BACKUP_RESPONSE_OPENAI_API_KEY=your_openai_api_key_here
BACKUP_RESPONSE_OPENAI_BASE_URL=https://api.openai.com/v1
BACKUP_RESPONSE_OPENAI_MODEL=openai/gpt-4.1-nano

# Fake response fallback (app/tools/fake_response.py)
# Separate OpenAI-compatible endpoint for format-only fake responses
FAKE_RESPONSE_OPENAI_API_KEY=your_openai_api_key_here
FAKE_RESPONSE_OPENAI_BASE_URL=https://api.openai.com/v1
FAKE_RESPONSE_OPENAI_MODEL=openai/gpt-4.1-nano

# Completion token caps (help control cost)
# Global cap for completion tokens across providers
LLM_MAX_OUTPUT_TOKENS=800
# Provider-specific overrides (optional)
OPENAI_MAX_OUTPUT_TOKENS=800
OPENROUTER_MAX_OUTPUT_TOKENS=800
GEMINI_MAX_OUTPUT_TOKENS=800

# Sandbox Configuration
USE_SANDBOX=true
SANDBOX_MODE=docker  # "docker" or "uv"
# SANDBOX_DOCKER_IMAGE=data-agent-sandbox:latest

# Server Configuration
HOST=0.0.0.0
PORT=8000

# Orchestration & Token Management (advanced)
# Limit size of tool results added back to LLM context
MAX_FUNCTION_RESULT_CHARS=20000
LARGE_FUNCTION_RESULT_CHARS=10000
# Extract large inline data from user prompts to shared_results
MAX_INLINE_DATA_CHARS=4000
# Nudge the model to be brief
MAX_OUTPUT_WORDS=200

# Request Timeout
REQUEST_TIMEOUT=170

